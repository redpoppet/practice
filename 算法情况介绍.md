# 算法目前主要工作和近期计划：


## 1：模型测试打包部署：
    模型合并，测试，加密，打包，部署等整套流程的实施，现在正在进行CI/CD 自动化。 产品中已经基本实现了，项目中还在探索。因为涉及到vpn。

## 2：组件库介绍
    每个模型都有数据读取，数据处理，特征准备，模型决策等过程，目前是模型间独立的。为了实现模型开发效率的提升，
    通过提取每个模型的每个模块的通用功能，加以抽象，意在形成一套辅助模型开发的基础组件，提升代码复用和开发效率。

## 3：异构集群开发与调度
    目前的计算任务从类别上可以分为数仓任务和模型任务，从运行方式上可以分为跑批任务或实时任务，目前数仓任务主要在集群内节点运行，模型任务在集群外机器上单独运行。
    未来打算将模型机器节点加入集群，然后采用yarn的任务队列和节点标签功能，来限制提交任务的执行资源和执行节点。实现yarn对资源的统一管理和调度，更方便未来租户增多，机器可以随意扩展。


## 4：加密方式
    目前主要是通过将python文件编译成so文件，进行加密，不过偶尔会出现加密后运行结果有出入问题，推断是Cython的版本和其他依赖包的版本问题。
    目前的解决办法是把那一两个造成差异影响的文件不加密。


## 5: 补调模型优化
    目前补调模型没有集群扩展思路，只能每台机器部署一个品牌。

    1：将数据处理的内容下沉到数据库计算，将目标库存修改为规则模型，采用数仓进行计算。

    2：核心求解部分使用其他语言加以改造。
    

## 6：模型前置数据校验和模型后置指标评估

    1：对于模型的数据输入数据进行严格的定义和检验。包含字段名，表名，数据类型，数据量趋势等四个方面。目前仅仅实现了字段级别的检测，且在测试岗位实施。

    2：对于模型的实施后结果评判，目前是由数据分析师或者建模单独完成，能够集成到组件中，对于模型中的每个模块都有一个比较明确的评价体系。


# 算法目前的挑战：
- 加密方案是否有更优的方式

- 实时模型与跑批模型的架构融合

- 弹性资源支撑是否一定要用Docker + Kubenetes，如何更方便的做资源扩展?

- 线上模型如果优雅地支持部署，更新，删除，回退,灰度部署等操作？


# 目前工程化和模型碰到的主要问题：
1： 部署：目前是部署代码，不是部署训练后的模型文件，线上也需要训练。

2： 多机扩展： 太平鸟模型部署架构，受限于资源，每次扩展都需要修改下

3： 部署新模型需要重新运行模型补历史数据：算法手动跑，历史数据。

4： 模型代码的合并，算法一直在合并建模的代码，建模没有基于合并后的代码开
发，所以偶尔会存在算法版本结果和建模版本结果不一致的情况，重新比对耗时较长，效率低下。

5:  效率问题，比如读数的效率问题，在循环内读数和在循环外读数的效率选取问题。

6： 季节，年份，类别特征硬编码，导致季节变更时代码报错。

7： 模型溯源查错时，需要回复到当时的状态，需要保存现场数据，目前是在线上存储为pickle文件造成数据存储量大，循环删除时间不太好固定。且建模人员没有权限拉取线上文件，需要运维代劳，效率比较低。

8： 模型重新上线，需要重新训练，导致线上回归时间比较长。

9： 主线版本模型的更新，需要所有项目模型代码更新。

10： 模型测试时间较长，如果涉及到多个品牌的话，因为每个品牌的代码不一致，需要全部测试。开发自测时和测试测试时，部分条件不一致，导致效果验证比较难。

11：新条件或新约束的测试比较难，基本上都要造数测试或者等待客户整理，效率低下，老出错。

12：客户给的数据，导入的数据，很容易存在缺失问题，导致模型很多时候有问题。目前都是在模型里面以excel，pickle方式提供，导致模型可移植性比较差。



